{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Disaster Tweet Classifier with DistilBERT and Custom Head\n",
    "\n",
    "The purpose of this model is to determine whether a given Tweet is about a real diaster (war, flood, famine, etc.) or benign. For example, the Tweet \"the sky looks beutifully ablaze tonight\" likelly does not refer to a real fire. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read from CSV\n",
    "dataset = pd.read_csv(\"./data/train.csv\").head(100)\n",
    "\n",
    "# Drop (potentially) unnecessary columns. These may be useful, but I'm not quite ready to work with missing data.\n",
    "dataset = dataset.drop([\"id\", \"keyword\", \"location\"], axis=1)\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Pretrained Model for Inference \n",
    "\n",
    "Below, I use the HuggingFace `transformers` library to fine-tune DistilBERT on the tweets dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, PreTrainedModel, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init tokenizer for converting text to numbers\n",
    "model_path = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# In order to add padding on a batch-level rather than a dataset level, add dynamic padding using a data \n",
    "# collator. This will add padding to the maximum input in a batch rather than the entire \n",
    "# data set which saves computation. \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Read data from CSV, embed, and split into test and train\n",
    "raw_dataset = Dataset.from_pandas(dataset)\n",
    "raw_dataset = raw_dataset.rename_column(\"target\", \"labels\")\n",
    "raw_dataset = raw_dataset.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
    "raw_dataset = raw_dataset.with_format(\"torch\")\n",
    "formatted_datasets = raw_dataset.train_test_split(0.2)\n",
    "\n",
    "# Show Output\n",
    "formatted_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitDetectionModel(PreTrainedModel):\n",
    "    def __init__(self, encoding_model, num_labels=2):\n",
    "        super(TraitDetectionModel, self).__init__(config=encoding_model.config)\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = encoding_model\n",
    "        input_dimension = encoding_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(input_dimension, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None):\n",
    "        encoding = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "        cls_tensor = encoding[1]\n",
    "        logits = self.classifier(cls_tensor)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            loss = loss_function(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=encoding.hidden_states,\n",
    "            attentions=encoding.attentions\n",
    "        )\n",
    "    \n",
    "    def save_pretrained(self, save_directory, state_dict = None):\n",
    "        self.encoder.save_pretrained(save_directory)\n",
    "        file_name = \"TraitDetectionModel-\" + self.encoder.base_model.base_model_prefix + \".pt\"\n",
    "        torch.save(self.state_dict(), save_directory + f\"/{file_name}\")\n",
    "    \n",
    "    def from_pretrained(model_path):\n",
    "        encoder = AutoModel.from_pretrained(model_path)\n",
    "        file_name = \"TraitDetectionModel-\" + encoder.base_model.base_model_prefix + \".pt\"\n",
    "        new_model = TraitDetectionModel(encoder)\n",
    "        state_dictionary = torch.load(model_path + f\"/{file_name}\")\n",
    "        new_model.load_state_dict(state_dictionary)\n",
    "        return new_model\n",
    "   \n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = TraitDetectionModel(AutoModel.from_pretrained(model_path))\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Running on Device Type: {device.type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\"test-trainer\", num_train_epochs=1)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_arguments,\n",
    "    train_dataset=formatted_datasets[\"train\"],\n",
    "    eval_dataset=formatted_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Combined Method\n",
    "model.save_pretrained(\"./models/custom-method/\")\n",
    "\n",
    "# Save PyTorch Method\n",
    "torch.save(model, \"./models/pytorch/full_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_loaded = torch.load(\"models/pytorch/full_model.pt\") # Load Pytorch Model\n",
    "combined_loaded = TraitDetectionModel.from_pretrained(\"models/custom-method\") # Load Combined Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pytorch_loaded.eval()\n",
    "combined_loaded.eval()\n",
    "\n",
    "inputs = tokenizer(\"The sky is ablaze\", return_tensors=\"pt\")\n",
    "print(model(**inputs)) # The original BERT model with custom classifier\n",
    "print(pytorch_loaded(**inputs)) # The model that was loaded only from the local PyTorch file\n",
    "print(combined_loaded(**inputs)) # The model that was loaded from the local PyTorch and hugigng face config using a combined approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.equal(model.encoder(**inputs)[0][:, 0, :], model.encoder(**inputs)[0][:, 0, :]))\n",
    "print(torch.equal(model.encoder(**inputs)[0][:, 0, :], pytorch_loaded.encoder(**inputs)[0][:, 0, :]))\n",
    "print(torch.equal(model.encoder(**inputs)[0][:, 0, :], combined_loaded.encoder(**inputs)[0][:, 0, :]))\n",
    "print(torch.equal(model.classifier.weight, pytorch_loaded.classifier.weight))\n",
    "print(torch.equal(model.classifier.weight, combined_loaded.classifier.weight))\n",
    "print(torch.equal(model.encoder(**inputs)[1], model.encoder(**inputs)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.nn.functional.softmax(model(**inputs).logits))\n",
    "print(torch.nn.functional.softmax(pytorch_loaded(**inputs).logits))\n",
    "print(torch.nn.functional.softmax(combined_loaded(**inputs).logits))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bed1c0c4dec85a904206baf43aa94359ab21b6b8032fc9ee47d5e9164a02932c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('disaster-tweet-classifier-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
